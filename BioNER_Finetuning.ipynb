{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b073c4-dd74-44cb-b021-f0791892f703",
   "metadata": {},
   "source": [
    "# End-To-End Named Entity Recognition (NER) with Ray and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adc1f6-365f-40d4-9bd6-16e10087d18f",
   "metadata": {},
   "source": [
    "This notebook takes you through an end-to-end NER use-case using Ray's distributed processing, training, and serving capabilities. This notebook utilizes the [BioNER](https://github.com/dmis-lab/biobert?tab=readme-ov-file) dataset. By the end of this notebook, you will have:\n",
    "- Performed standard NER data processing steps like tokenization, lemmatization using Ray Core and Ray Data\n",
    "- Fine-tuned the base [distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) using Ray Train\n",
    "- Gained an understanding how to use Ray Train to configure your model training, evaluation metrics, checkpointing, etc.\n",
    "- Utilized Ray Serve to create an endpoint for your model that offers fast inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecffe3-712e-443a-ba26-f60c5407d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1a0d0-bfc5-444d-bd16-5481b03d9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logging\n",
    "import logging\n",
    "logging.getLogger().disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f553cf-8101-444b-86e6-19212247fe24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72631ff1-4c62-40ae-a375-c5d6491823e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View resources available to Ray - this should match your machine's hardware configuration\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626fedb5-4125-44e4-8a4c-c04f74db773c",
   "metadata": {},
   "source": [
    "## Load BioNER datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c93454-2f3f-4898-a3bc-e81b5e868aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_paths = glob.glob(\"./NERData/**/train.tsv\", recursive=True)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8a6eb-ba41-47e8-a4ed-1b22b4df0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions to read tsv file - sourced from https://github.com/dmis-lab/biobert?tab=readme-ov-file\n",
    "def _read_tsv_data(input_file, fetch_limit = 100):\n",
    "    \"\"\"Reads a BIO data. Use fetch_limit to limit the number of l\"\"\"\n",
    "    inpFilept = open(input_file)\n",
    "    lines = []\n",
    "    words = []\n",
    "    labels = []\n",
    "\n",
    "    counter = 0\n",
    "    for lineIdx, line in enumerate(inpFilept):\n",
    "        contents = line.splitlines()[0]\n",
    "        lineList = contents.split()\n",
    "        if len(lineList) == 0: # For blank line\n",
    "            if counter > fetch_limit - 1:\n",
    "                break\n",
    "            assert len(words) == len(labels), \"lineIdx: %s,  len(words)(%s) != len(labels)(%s) \\n %s\\n%s\"%(lineIdx, len(words), len(labels), \" \".join(words), \" \".join(labels))\n",
    "            if len(words) != 0:\n",
    "                wordSent = \" \".join(words)\n",
    "                labelSent = \" \".join(labels)\n",
    "                lines.append((labelSent, wordSent))\n",
    "                words = []\n",
    "                labels = []\n",
    "                counter += 1\n",
    "            else: \n",
    "                print(\"Two continual empty lines detected!\")\n",
    "        else:\n",
    "            words.append(lineList[0])\n",
    "            labels.append(lineList[-1])\n",
    "            \n",
    "    if len(words) != 0 and counter < (fetch_limit - 1):\n",
    "        wordSent = \" \".join(words)\n",
    "        labelSent = \" \".join(labels)\n",
    "        lines.append((labelSent, wordSent))\n",
    "        words = []\n",
    "        labels = []\n",
    "\n",
    "    inpFilept.close()\n",
    "    return lines\n",
    "\n",
    "# Wrapping this function as a ray task for experimentation\n",
    "@ray.remote\n",
    "def _read_tsv_data_remote(input_file, fetch_limit):\n",
    "    return _read_tsv_data(input_file, fetch_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235f988-4527-4d3e-b29a-98ae9ba68314",
   "metadata": {},
   "source": [
    "*The below cell contains a sample of what the loaded dataset looks like. BioNER is a collection of 8 smaller datasets (each covering different topics), denoted by the 8 different folders - each has a train.tsv file, which contains a collection of sentences and NER tags. The three NER tags are:*\n",
    "- B (Beginning): Indicates the first token of a named entity (biology related entities, in this case).\n",
    "- I (Inside): Marks subsequent tokens inside the same named entity.\n",
    "- O (Outside): Denotes tokens that do not belong to any named entity\n",
    "\n",
    "*The helper function above converts these files into arrays of tuples, where tuple[0] = NER tags, and tuple[1] = sentence corresponding to those tags*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c178550-b8a0-4dfe-be2a-9b49207ab40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to increase/decrease size of all datasets used downstream\n",
    "fetch_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313628f2-789d-4df4-b0df-54bfc30b0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample output\n",
    "sample_output = _read_tsv_data(file_paths[0], fetch_limit)\n",
    "print(sample_output[:3])\n",
    "len(_read_tsv_data(file_paths[0], fetch_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6bf4d-e3a1-49ca-8632-cbb09fc5f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Native Python version - this is a single threaded, sequential way of reading the files\n",
    "for file in file_paths:\n",
    "    _read_tsv_data(file, fetch_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1257350-3bc2-4984-9307-dc149a83e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ray task version - notice how CPU time is much lower in this case. This is because Ray automatically distributes over 'workers'\n",
    "futures = [_read_tsv_data_remote.remote(file, fetch_limit) for file in file_paths]\n",
    "_ = ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d01c08-e92d-4d3e-9890-a6cea268d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using Ray data to provide a lazy-evaluatable interface to our training data\n",
    "from typing import Any, Dict\n",
    "def parse_file(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\"parsed_file\": _read_tsv_data(row['file_path'], fetch_limit)} # note: standalone python objs not allowed in Ray data!\n",
    "\n",
    "ray_ds = ray.data.from_items([{\"file_path\": path} for path in file_paths])\n",
    "processed_ds = ray_ds.map(parse_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b95e00-109e-473e-837a-21d1a7ab1b18",
   "metadata": {},
   "source": [
    "*(Depends on fetch_limit - for small sizes, native Python is quicker as Ray adds overhead) As seen above, the Ray task version performs best, as it essentially reads the input training files in parallel. For the purposes of this exercise, however, we will be going with the Ray data version as it more closely resembles what one would do for a larger/production scale dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723666e-fe33-4f72-bed1-f64a5a26663e",
   "metadata": {},
   "source": [
    "### Lemmatize and Tokenize Data using Ray\n",
    "After we've loaded data, the next step is to to perform some processing on it to make it more useful to the model. Specifically, we will be performing:\n",
    "- Lower-casing the sentences\n",
    "- Lemmatization, i.e., converting words to their root form. For example, cats would be converted to cat. This helps remove distractions and improves language model understanding\n",
    "- Converting sentences to inputs the LLM will understand (i.e., tokens) using the appropriate tokenizer model. We also pad the sentences to the max length accepted by the model to allow us to use multiple batches per forward pass in our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50256a-77e1-4d3f-8b8f-44147e732c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab', download_dir='C:\\\\Users\\\\Varun Jadia\\\\Desktop\\\\coding_assignments\\\\ray\\\\ray_venv\\\\nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:\\\\Users\\\\Varun Jadia\\\\Desktop\\\\coding_assignments\\\\ray\\\\ray_venv\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aced17-5d1b-4168-b78a-3979fb9bfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nltk lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentence = \"The cats are sitting on the bed.\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01abbcc-8ca5-4937-a0fe-5fb72896473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokenize_and_align_labels(batch):\n",
    "    from transformers import AutoTokenizer\n",
    "    import torch\n",
    "    \n",
    "    # Simple dict to map NER tags to categorical variables\n",
    "    label_to_int = {\n",
    "        'B': 0,\n",
    "        'I': 1,\n",
    "        'O': 2\n",
    "    }\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    max_length = 512\n",
    "    \n",
    "    parsed_files = batch['parsed_file']\n",
    "    tokenized_inputs = []\n",
    "    \n",
    "    for parsed_file in parsed_files:\n",
    "        for i, (label_str, sentence) in enumerate(parsed_file):\n",
    "            label_list = label_str.split()\n",
    "            words = sentence.lower().split()  # Convert to lower case\n",
    "            \n",
    "            if len(label_list) != len(words):\n",
    "                raise ValueError(f\"Mismatch: {len(label_list)} labels but {len(words)} words\")\n",
    "            \n",
    "            input_ids = []\n",
    "            aligned_labels = []\n",
    "            \n",
    "            for word, label in zip(words, label_list):\n",
    "                lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                word_tokens = tokenizer.tokenize(lemmatized_word)\n",
    "                \n",
    "                if not word_tokens:\n",
    "                    continue\n",
    "                \n",
    "                word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "                input_ids.extend(word_ids)\n",
    "                aligned_labels.append(label_to_int[label])\n",
    "\n",
    "                if len(input_ids) > max_length - 2:\n",
    "                    break # early break if > seq length\n",
    "                \n",
    "                if len(word_tokens) > 1:\n",
    "                    if label == 'B':\n",
    "                        remaining_label = 'I'\n",
    "                    else:\n",
    "                        remaining_label = label  # Either 'I' or 'O'\n",
    "                    \n",
    "                    for _ in range(len(word_tokens) - 1):\n",
    "                        aligned_labels.append(label_to_int[remaining_label])\n",
    "            \n",
    "            # Truncate if longer than max_length (accounting for special tokens)\n",
    "            if len(input_ids) > max_length - 2:  # -2 for [CLS] and [SEP]\n",
    "                input_ids = input_ids[:max_length - 2]\n",
    "                aligned_labels = aligned_labels[:max_length - 2]\n",
    "            \n",
    "            final_input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n",
    "            final_labels = [-100] + aligned_labels + [-100]\n",
    "            attention_mask = [1] * len(final_input_ids)\n",
    "\n",
    "            # Padding any sentences smaller than max_length\n",
    "            padding_length = max_length - len(final_input_ids)\n",
    "            if padding_length > 0:\n",
    "                final_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                attention_mask += [0] * padding_length\n",
    "                final_labels += [-100] * padding_length\n",
    "            \n",
    "            tokenized_inputs.append({\n",
    "                'input_ids': final_input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': final_labels\n",
    "            })\n",
    "    \n",
    "    return {\"tokenized_inputs\": tokenized_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58421448-ddb5-478b-aae3-b8df71dcece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = processed_ds.map_batches(lemmatize_tokenize_and_align_labels, batch_size=10)\n",
    "tokenized_ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3bf63a-e6e7-4952-b466-6e6c6eedd19a",
   "metadata": {},
   "source": [
    "*Note that the Ray Data only materializes/evaluates the data when requested, as in the below cell using take_batch (there are other APIs that allow for accessing data from a Ray Data object, like to_pandas(), etc.). Also note how you can chain transformations from one data object to another using map_batches*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6c799-37f0-48d7-839f-0f65fbac66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect tokenized_ds\n",
    "check_batches = tokenized_ds.take_batch(batch_size=2)\n",
    "len(check_batches['tokenized_inputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02c44c-55bb-4f54-b851-6de5d1d83625",
   "metadata": {},
   "source": [
    "### Finetune DistilBERT on BioNER dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587bf44-f60f-415c-a53c-ac6b0aaefd0c",
   "metadata": {},
   "source": [
    "*The goal of this section is to finetune the DistilBERT model (a smaller and faster version of the canonical BERT model) on the BioNER dataset to improve performance. We'll analyze the base model's performance (which we expect to be bad) first before running our fine-tuning loop. For this exercise, we will only be fine-tuning the classification head of the model to make our weight updates quicker*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89b6d1-1eb1-4145-bfd7-a6d2024367e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray data connects directly to torch dataloader\n",
    "# NOTE: Ensure using torch 2.3.0 to ensure libuv backend is not used\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.int64),\n",
    "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.int64),\n",
    "            'labels': torch.tensor(item['labels'], dtype=torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc2c12-900e-4b38-a3f7-3907c7030b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(TokenizedDataset(check_batches['tokenized_inputs']), batch_size=1)\n",
    "for data in dl:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d73db-ba50-4f3c-bacc-24628cbaf40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to('cpu')\n",
    "model(input_ids=data['input_ids'], attention_mask=data['attention_mask'], labels=data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054b1d1-c1ea-4670-8f4b-8c1951fcae21",
   "metadata": {},
   "source": [
    "*An important part of the process is to define metrics to calculate the performance of our model. As ours is a classification task (we are classifying tokens into 1 of 3 entities), the metrics we will use are:*\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Accuracy\n",
    "\n",
    "We calculate these using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2022d-185d-4e18-91c0-226a5d8b1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate precision, accuracy, f1score\n",
    "def evaluate_token_classification(model, dataloader):\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    confusion = torch.zeros(3, 3, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to('cpu')\n",
    "            attention_mask = batch['attention_mask'].to('cpu')\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu()\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                mask = attention_mask[i].cpu().bool()\n",
    "                pred_tokens = preds[i][mask]\n",
    "                label_tokens = labels[i][mask]\n",
    "                \n",
    "                for true_label, pred_label in zip(label_tokens, pred_tokens):\n",
    "                    if not (true_label == -100 or pred_label == -100):\n",
    "                        confusion[true_label, pred_label] += 1\n",
    "    \n",
    "    total_samples = confusion.sum().item()\n",
    "    correct_predictions = confusion.diag().sum().item()\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    class_metrics = {}\n",
    "    for class_idx in range(3):\n",
    "        true_positives = confusion[class_idx, class_idx].item()\n",
    "        false_positives = confusion[:, class_idx].sum().item() - true_positives\n",
    "        false_negatives = confusion[class_idx, :].sum().item() - true_positives\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics[class_idx] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'class_metrics': class_metrics,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "evaluate_token_classification(model, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a7630-9f2c-44b9-ad8c-7e0af9a6e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze bert parameters and only allow updates for classification head - this is to reduce the number of parameters we are updating\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eaf9f-86d6-4903-9d0f-43b3f7699380",
   "metadata": {},
   "source": [
    "*Now we create a training loop using the Ray Train framework, which allows for distributed training using a simple ScalingConfig, TrainerConfig - Ray dynamically splits the training set among workers, manages weight updates between workers, etc. to allow you to speed up training by running it over several cores.\n",
    "Recall that typically in pytorch, the training loop is run as a simple for loop. Here's an example:*\n",
    "\n",
    "```\n",
    "for epoch in range(2): # number of times loop over train set\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # calculate gradient updates, i.e., backprop\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:    # print every 10 mini-batches\n",
    "            val_accuracy = eval_model(net, valloader)\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}, val accuracy: {val_accuracy}')            \n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fad862-d0ad-4411-808e-a87f458abce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that limit still returns a Dataset\n",
    "train_set, val_set = tokenized_ds.train_test_split(test_size=0.30)\n",
    "train_set.count(), val_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1744db1-a2fc-4cb6-890a-1c9062b0f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig, Checkpoint\n",
    "import ray\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    model_name = config[\"model_name\"]\n",
    "    num_labels = config[\"num_labels\"]\n",
    "    epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    \n",
    "    train_examples, val_examples = [], []\n",
    "    train_data = ray.train.get_dataset_shard(\"train\")\n",
    "    val_data = ray.train.get_dataset_shard(\"val\")\n",
    "\n",
    "    if train_data is None or val_data is None:\n",
    "        raise ValueError(\"Dataset shard is None. Ensure dataset is passed correctly to TorchTrainer.\")\n",
    "\n",
    "    train_examples, val_examples = [], []\n",
    "    \n",
    "    for batch in train_data.iter_batches():\n",
    "        train_examples.extend(batch[\"tokenized_inputs\"])\n",
    "\n",
    "    for batch in val_data.iter_batches():\n",
    "        val_examples.extend(batch[\"tokenized_inputs\"])\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    train_dataset = TokenizedDataset(train_examples)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss # Note: CrossEntropy loss is automatically calculated\n",
    "            \n",
    "            # Backward pass, ensure you zero out gradients first\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_accuracy = evaluate_token_classification(model, train_dataloader)['accuracy']\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "\n",
    "        # Save checkpoint only from the rank 0 worker to prevent redundant checkpoints\n",
    "        if ray.train.get_context().get_world_rank() == 0 and (epoch % config[\"checkpoint_freq\"]) == 0:\n",
    "            checkpoint_dir = os.path.join(config[\"checkpoint_dir\"], f\"epoch_{epoch + 1}\")\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"model.pt\"))\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir) # wrapper around torch serialization\n",
    "        \n",
    "            val_dataset = TokenizedDataset(val_examples)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_accuracy = evaluate_token_classification(model, val_dataloader)['accuracy']\n",
    "            \n",
    "            ray.train.report({\"loss\": avg_loss, \"val_accuracy\": val_accuracy, \"train_accuracy\": train_accuracy}, checkpoint=checkpoint)\n",
    "        else:\n",
    "            ray.train.report({\"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230631db-5483-4287-b0e3-0004245d17d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define training configuration\n",
    "train_config = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"num_labels\": 3,  # B, I, O\n",
    "    \"num_epochs\": 2,\n",
    "    \"batch_size\": 10, # pytorch batch size, keep small due to system constraints\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"checkpoint_dir\": 'C:\\\\Users\\\\Varun Jadia\\\\Desktop\\\\coding_assignments\\\\ray\\\\ner_project\\\\distilbert',\n",
    "    \"checkpoint_freq\": 1 # 1 = save checkpoint every epoch\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, # scale up as necessary\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": train_set, \"val\": val_set},\n",
    "    run_config=RunConfig(\n",
    "        name=\"biobert_ner_training\",\n",
    "        storage_path='C:\\\\Users\\\\Varun Jadia\\\\Desktop\\\\coding_assignments\\\\ray\\\\ner_project\\\\ray_results'\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = trainer.fit()\n",
    "print(f\"Training complete. Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a4b23-f6b8-41c2-a426-b0880867cc29",
   "metadata": {},
   "source": [
    "*Ray also has a checkpointing feature that allows us to save the model’s state at regular intervals. In the training loop, we save checkpoints only on the main worker (world_rank == 0) at a specified frequency (checkpoint_freq). The model’s state dictionary is stored in the configured directory, and Ray’s Checkpoint.from_directory() registers it with Ray Train for tracking. This ensures efficient checkpointing without redundant saves across distributed workers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae9950-8262-4da6-a340-2e5fe6c27c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_checkpoint = results.checkpoint\n",
    "best_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=3\n",
    ")\n",
    "best_model.load_state_dict(torch.load(os.path.join(final_checkpoint.path, \"model.pt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566b1e5-6345-442c-a809-2daeaa3da635",
   "metadata": {},
   "source": [
    "### Get test data metrics\n",
    "*We now evaluate our finetuned model on test data. To do this, we build a test dataset using our previously defined functions, now applied to test.tsv files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60f3fe-15fa-4d0a-99a7-7e8d51385c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_paths = glob.glob(\"./NERData/**/test.tsv\", recursive=True)\n",
    "test_ds = ray.data.from_items([{\"file_path\": path} for path in test_file_paths])\n",
    "processed_test_ds = test_ds.map(parse_file)\n",
    "tokenized_test_ds = processed_test_ds.map_batches(lemmatize_tokenize_and_align_labels, batch_size=2)\n",
    "tokenized_test_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863f792-a701-4037-b582-9b1f8eb9237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "test_examples = []\n",
    "for batch in tokenized_test_ds.iter_batches():\n",
    "    test_examples.extend(batch[\"tokenized_inputs\"])\n",
    "\n",
    "test_dataset = TokenizedDataset(test_examples)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "evaluate_token_classification(best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c3ebc-33e3-4359-a2f8-0692f5b568de",
   "metadata": {},
   "source": [
    "*Note that model accuracy/other metrics are likely to be bad if trained on a limited number of samples/few epochs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621eab0e-8df0-4368-ab92-68731af026e3",
   "metadata": {},
   "source": [
    "### Using Ray tune to train learning rate\n",
    "*Hyperparameter tuning is a standard part of any ML workflow - Ray also provides an interface to do this in a similar fashion to our training setup above. For this example, we'll find the optimal learning rate for our model above by giving Ray a search space to look over. Some key features of Ray tune being used here:*\n",
    "- ASHAScheduler: controls how trials are terminated early to save computational resources\n",
    "- BayesOptSearch: This is the search algorithm we use here by Ray to select the next lr value to try. Generally better than a simple grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289360c-87e5-4996-8bda-754efc3eda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "def tune_learning_rate(num_samples):\n",
    "    search_space = {\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "    }\n",
    "    \n",
    "    base_config = {k: v for k, v in train_config.items() if k != \"learning_rate\"} # learning_rate will now come from Tune\n",
    "    tune_config = {**base_config, **search_space}\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=train_config[\"num_epochs\"],\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "        metric=\"val_accuracy\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "\n",
    "    search_alg = BayesOptSearch(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "    def trainable(config):\n",
    "        # The config passed to this function will include the sampled learning_rate\n",
    "        trainer = TorchTrainer(\n",
    "            train_loop_per_worker=train_loop_per_worker,\n",
    "            train_loop_config=config,\n",
    "            scaling_config=scaling_config,\n",
    "            datasets={\"train\": train_set, \"val\": val_set},\n",
    "        )\n",
    "        result = trainer.fit()\n",
    "        return result\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        trainable=trainable,\n",
    "        param_space=tune_config,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "            search_alg=search_alg,\n",
    "            trial_dirname_creator=trial_dirname_creator\n",
    "        ),\n",
    "        run_config=RunConfig(\n",
    "            name=\"learning_rate_tuning\",\n",
    "            storage_path=\"C:\\\\Users\\\\Varun Jadia\\\\Desktop\\\\coding_assignments\\\\ray\\\\ner_project\\\\ray_results\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(metric=\"val_accuracy\", mode=\"max\")\n",
    "    best_config = best_result.config\n",
    "    best_checkpoint = best_result.checkpoint\n",
    "    \n",
    "    return best_config, best_checkpoint\n",
    "\n",
    "# we try only 2 different learning rates in this example...\n",
    "best_config, best_checkpoint = tune_learning_rate(num_samples=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed18536-6e73-443d-bc9d-7d58b94f2148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
